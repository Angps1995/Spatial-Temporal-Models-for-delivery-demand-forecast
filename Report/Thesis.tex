\documentclass[nonblindrev,msom]{informs3} % format for MS submission
\usepackage{amsmath,dcolumn,booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
% Natbib setup for author-year style
\usepackage{natbib}
\usepackage{color}
\usepackage{epstopdf} % Allow importing eps file directly
\bibpunct[, ]{(}{)}{,}{a}{}{,}%
\def\bibfont{\small}%
\def\bibsep{\smallskipamount}%
\def\bibhang{24pt}%
\def\newblock{\ }%
\def\BIBand{and}%

\makeatletter
\renewcommand*\env@matrix[1][c]{\hskip -\arraycolsep
	\let\@ifnextchar\new@ifnextchar
	\array{*\c@MaxMatrixCols #1}}
\makeatother

%% Setup of theorem styles. Outcomment only one.
%% Preferred default is the first option.
\TheoremsNumberedThrough     % Preferred (Theorem 1, Lemma 1, Theorem 2)
%\TheoremsNumberedByChapter  % (Theorem 1.1, Lema 1.1, Theorem 1.2)
\ECRepeatTheorems

%% Setup of the equation numbering system. Outcomment only one.
%% Preferred default is the first option.
\EquationsNumberedThrough    % Default: (1), (2), ...
%\EquationsNumberedBySection % (1.1), (1.2), ...


%\pagestyle{plain}
%\setlength{\parskip} {12pt}
%\setlength{\parindent} {2em}
%\setlength{\textwidth}{6.5in}
%\setlength{\oddsidemargin} {0in}
%\setlength{\textheight} {8.5in}
%\setlength{\topmargin} {-0.5in}
%\parskip 7.2pt



% For new submissions, leave this number blank.
% For revisions, input the manuscript number assigned by the on-line
% system along with a suffix ".Rx" where x is the revision number.
\MANUSCRIPTNO{}
\newcommand{\vocab}[1]{\textcolor{red}{#1}}
\linespread{1.3}
%%%%%%%%%%%%%%%%
\usepackage{float}
\DeclareUnicodeCharacter{2212}{-}
\begin{document}
%%%%%%%%%%%%%%%%


% Outcomment only when entries are known. Otherwise leave as is and
%   default values will be used.
%\setcounter{page}{1}
%\VOLUME{00}%
%\NO{0}%
%\MONTH{Xxxxx}% (month or a similar seasonal id)
%\YEAR{0000}% e.g., 2005
%\FIRSTPAGE{000}%
%\LASTPAGE{000}%
%\SHORTYEAR{00}% shortened year (two-digit)
%\ISSUE{0000} %
%\LONGFIRSTPAGE{0001} %
%\DOI{10.1287/xxxx.0000.0000}%

% Author's names for the running heads
% Sample depending on the number of authors;
% \RUNAUTHOR{Jones}
% \RUNAUTHOR{Jones and Wilson}
% \RUNAUTHOR{Jones, Miller, and Wilson}
% \RUNAUTHOR{Jones et al.} % for four or more authors
% Enter authors following the given pattern:
\RUNAUTHOR{Ang Peng Seng}

% Title or shortened title suitable for running heads. Sample:
% \RUNTITLE{Bundling Information Goods of Decreasing Value}
% Enter the (shortened) title:
\RUNTITLE{NUS Business School Honors Dissertation}

% Full title. Sample:
% \TITLE{Bundling Information Goods of Decreasing Value}
% Enter the full title:
\TITLE{NUS Business School Honors Dissertation}

% Block of authors and their affiliations starts here:
% NOTE: Authors with same affiliation, if the order of authors allows,
%   should be entered in ONE field, separated by a comma.
%   \EMAIL field can be repeated if more than one author
\ARTICLEAUTHORS{%
			\AUTHOR{Peng Seng Ang}
			\AFF{National University of Singapore\\
				\EMAIL{ang.peng.seng@u.nus.edu}}
	% Enter all authors
}

\ABSTRACT{Many real industry problems involves spatio-temporal demand prediction, which re-
quires predicting demand at a certain time across different locations. Most of the
demand data are not continuous variables but instead count variables, like number
of orders and number of transactions. Previous research have mainly explored neural
network methods for spatio-temporal problems. In this paper, we focus on how we can
apply classical time series model, using Vector Autoregressive models to exploit spatial
correlations as well as introduce a 3-step approach to improve forecast accuracy.}


% Fill in data. If unknown, outcomment the field
\KEYWORDS{spatio-temporal demand prediction, time series}
\maketitle

% Text of your paper here


\section{Introduction}

This paper would explore the usage of classical statistical methods along with unsupervised learning methods, like clustering, for the purpose of spatio-temporal forecasting. The motivation for this paper comes from \cite{Sheng2018}, where the focus is to optimise assignments of lunch delivery orders to drivers in order to minimize the total delay of all drivers. The dataset used in both \cite{Sheng2018} and this paper are from a food service provider in China that allows customer to place orders before a cutoff time in the day (e.g 10.00am) and the customer can expect to recieve their orders by a deadline (e.g anytime from 10.30am to 11.45am). \\

\noindent This problem is not only restricted to the abovementioned food provider. With the rise of e-commerce and the food ordering and delivery services, like GrabFood and FoodPanda, demand prediction and driver assignment problem would be an everyday concern for them too. Spatio-temporalal forecasting is also not limited to food delivery, and it has many other applications, like traffic prediction and weather prediction over time. \\

\noindent In reality, demand is never deterministic and hence, having an accurate forecast of demand for the food service providers would help them more effectively and efficiently assign orders to drivers to improve the overall delivery time. As such, the focus of this paper would be to accurately model and forecast the demand at the different locations and time. Currently, most Autoregressive (AR) or Autoregressive Integrated Moving Average (ARIMA) models only consider temporal features when predicting demand. However, we believe including spatial features between the data points might improve forecast accuracy. In this paper, classical time series models, such as Generalized Linear Model (GLM), ARIMA, as well as Vector Autoregressive (VAR) models, that include both spatial and temporal features would be explored. We also introduce a 3-step approach that combined unsupervised learning and classical statistical methods to improve forecast accuracy. 

\section{Literature Review}

\noindent Spatio-temporal demand prediction has slowly gained popularity over the years with the rise of deep learning. There are various research on different deep learning methods used for spatio-temporal demand prediction. One example would be \cite{Wang2018}, where they applied deep spatio-temporal convolutional LSTM methods for traffic demand prediction. \\

\noindent While most existing research regarding spatio-temporal forecasting makes use of deep learning methods, this paper would be focus only on using classical statistical method instead as the dataset we are using is small and classical methods are relatively more interpretable than deep learning neural networks. Another example of using classical time series methods for spatio-temporal forecasting would be \cite{Abolfazl2017}, where they proposed using generalized spatio-temporal autoregressive (STAR) model for predicting taxi demands across locations in New York City. \cite{KURT20152537} also provides the background and theoretical details of STAR models, which they used on a dataset of regional bank deposits. \\

\noindent \cite{Marina2016} describes and shows how they implemented a network autoregressive moving average model to model the number of cases of Mumps in UK counties. In their example, they also showed that they might achieve a better result by modelling the series separately as univariate time series, also suggested in \cite{Nunes2015} since the neighbouring counties does not provide a substantial amount of explanatory power.  Other related literature includes \cite{Xavier2005} where they propose a model building strategy for spatially sparse but temporally rich data. \\

\noindent Since the dataset that we will be using in this paper have a unique feature in which our data are count, which are integer, values. As such, classical models such as poisson regression that are used for count data would produced probabilistic result in the form of fracional numbers. Hence, past studies related to the different metrics used to evaluate probabilistic forecasting was done. \cite{Czado2009PredictiveMA} mentioned that probabilistic forecasting should aim to maximise sharpness of predictive distributions subject to calibration, where sharpness is defined as the concentration of the predicted distribution while calibration refers to the statistical consistency between the forecasts and the actual observation. \\

\noindent BigVAR and tscount are some libraries that would be used in this paper. \cite{Tobias2017} provides the mathematical background and implementation of Generalised Linear Models (GLM) for count time series as a library (tscount) in R while \cite{William2017} extensively descibes the background and implementation of the VAR models and BigVAR library for multi-variate time series.



\section{Data}
The data source used was an operational dataset from a food delivery service provider from Shanghai that includes delivery information for a 2-month period from 10 August 2015 to 30 September 2015 (excluding Saturdays) in 2015. The provider only provides delivery service for 90 minutes during lunchtime and the dataset has split the data into 15-minute time periods, and as such, each day would only consists of demand data for 6 time periods. Hence, our dataset has 839 locations with demand count data, in integer, for 204 time periods in total. \\

\noindent To include other exogenous variables, data from https://www.worldweatheronline.com/shanghai-weather-history/shanghai/cn.aspx was used to include weather and rainfall data as well as encoding of the weekdays for all the respective days. 

\subsection{Exploratory Analysis}
We would first do some exploratory analysis and check if there are any interesting relationships between the variables. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth, height=0.28\textheight]{Images/example_all_ts.jpg}
    \textbf{\caption{Time series of all locations in the dataset. It can be observed that most locations have very low number of orders across the time period.}}
    \label{fig:Time series of all locations in the dataset. It can be observed that most locations have very low number of orders across the time period.}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=0.35\textheight] {Images/example_ts.jpg}
    \textbf{\caption{Most locations have very sparse time series (left) while some have relatively more dense time series (right)}}
    \label{fig:Most locations have very sparse time series (left) while some have relatively more dense time series (right)}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=0.35\textheight]{Images/boxplot_counts.jpg}
    \textbf{\caption{Boxplot of counts. We can see that most of the locations have extremely low number of non-zero orders and further analysis showed that about 335 locations have just a maximum of one non-zero order throughout the 204 time periods.}}
	\label{fig:Boxplot of counts. We can see that most of the locations have extremely low number of non-zero orders and further analysis showed that about 335 locations have just a maximum of one non-zero order throughout the 204 time periods.}
\end{figure}

\noindent Next, we observe how the demand changes across locations over time. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=0.75\textheight]{Images/spatial_demand.jpg}
    \textbf{\caption{Demand across locations over time. Plot suggests that the areas towards the right tend to have higher demand.}}
    \label{fig:Demand across locations over time. Plot suggests that the areas towards the right tend to have higher demand.}
\end{figure}


\noindent Weather, temperature, days are also some external factors that might affect the demand. For example, perhaps a day with higher temperature might see more demand for food delivery since people might not be as willing to go out for food. These exogenous variables would be used in our VARX model, which would be explained later in section 6.5. We performed some analysis on exogenous factors, such as the mean demand against temperature and against pressure. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth, height=0.32\textheight]{Images/temp_mean_demand.jpg}
    \textbf{\caption{Scatter plot of mean demand against temperature. }}
    \label{fig:Scatter plot of mean counts against temperature}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth, height=0.32\textheight]{Images/pressure_mean_demand.jpg}
    \textbf{\caption{Scatter plot of mean demand against pressure}}
    \label{fig:Scatter plot of mean counts against pressure}
\end{figure}

\noindent The scatter plot in Figure 4 visually display a slight positive relationship between temperature and mean demand across all locations whereas Figure 5 visually display a slight negative relationship between pressure and mean demand across all locations. We would explore usage and impact of the exogenous variables in predicting demand in the later portion of this paper. 



\subsection{Subsetting of data and train-test Split}
From Figure 1 in Section 3.1, the data is very sparse as there are many locations that have no demand counts for the majority of the time periods. Hence, to get a proof-of-concept model and a better idea of how our models perform, we first subset the dataset into a smaller one where we only consider locations with at least 50 non-zero counts across the time period, leaving us with 42 locations that meet this criteria. After the model is tested on this smaller dataset, we would evaluate the performance and then use the full dataset to test and fit our model. \\

\noindent To prevent overfitting of our model, the dataset was then split into training and test set by considering the first 33 days as the training set and the next 1 day as the test set. A period of 1 day was chosen as the test set duration because we are making a reasonable assumption that this model would only have to be run at the end of the day, hence only required to predict one-day ahead demand. \\

\noindent Our training set would then have 198 demand data for each location and test set would have 6 demand data for each location. \\

\noindent The following sections of the paper is structured as follows. We first implement a simple baseline ARIMA model in section 4. Section 5 and 6 follow a similar idea to the baseline model, but instead of fitting an ARIMA model on all locations, we fit a Generalized Linear Model (GLM) in Section 5 instead and in section 6, we fit a Vector Autoregressive Model using all the locations as variables. In section 7, we introduce a 3 step modelling approach which would be more interpretable and would also perform the best out of all the models. 

\subsection{Challenges with dataset}

There are 3 key challenges with our dataset:

	1. Sparse data as most of the demand values are 0
	
	2. Count data as all our values are in integer
	
	3. High-dimensional, where we have time series for 839 locations


\newpage

\section{Baseline Model}
In this section, we would build a simple baseline model by just fitting an ARIMA model on all the locations individually. In the later sections of the paper, we would try other approaches and compare the results against the baseline model. 

\subsection{Scoring Metrics Used}
The classical metric that would be used for comparison would be Mean Squared Error (MSE), which is calculated by:

\begin{center}
    %$\displaystyle MSE=\frac{1}{(T_2 - T_1 - 1)}\sum_{t=T_1}^{T_2 - 1}\left \| \hat{y}_{t+1} - y_{t+1} \right \|_{2}^{2}$
    $\displaystyle MSE=\frac{1}{n}\sum_{t=1}^{n}\left \| \hat{y}_{t} - y_{t} \right \|_{2}^{2}$
\end{center}
where \textit{n} is the number of data points, $\hat{y_t}$ is the predicted value at time t and ${y_t}$ is the actual value at time t. The aim is for our models to minimize the MSE in order to obtain a more accurate fit. \\

\noindent However, our dataset contains only count data and since most of the models used in this paper are producing probabilistic forecast, in fractional numbers, we should also consider other metrics used for evaluation of probabilistic forecasting for count data, as suggested in \cite{Czado2009PredictiveMA}. In this paper, we will also use the quadratic score (qs), which is defined as:

\begin{center}
    $\displaystyle qs(P, x) = -2p_{x} + \left \| p \right \|^{2}$
\end{center}

\noindent where P represents the predicted distribution while x represents actual observed value. We can also compute $\left \| p \right \|^{2} = \sum_{k=0}^{\infty }p_{k}^{2}$ using methods explained in Appendix A of \cite{Czado2009PredictiveMA}. The use of quadratic score in to evaluate time series models for counts was also suggested by \cite{jstor}. Quadratic score is also a strictly proper scoring rule as it follows the property:

\begin{center}
    $\displaystyle qs(x, x) \leq qs(P, x)$
\end{center}

\noindent For the quadratic score, our model should aim to minimize it.


\subsection{ARIMA models}
Autoregressive Integrated Moving Average (ARIMA) models are one of the most commonly used models for time series (\cite{Asha2016}). ARIMA models are made up of 3 processes, mainly the Autoregressive (AR) process, the Integrated (I) process and the Moving Average (MA) process (\cite{Jamal2018}). The AR process assumes that each observation can be expressed as a linear combination of its past values.  An AR(x) process would mean using x lagged values. The MA process assumes that each observation can be expressed as a linear combination of its current error term as well as its past error terms. The Integrated Process states that the time series can undergo differencing to ensure that the series is stationary. A MA(x) process would mean using x number of past error observations. Hence, an ARIMA model is usually represented by ARIMA(p,d,q), where \textit{p} represents the number of autoregressive terms, \textit{d} represents the number of differences needed for stationarity, and \textit{q} represents the number of lagged forecast errors. 

\noindent The general equation of a ARIMA(p,d,q) is: 
\begin{center}
$\hat{y_t} = \mu + \sum_{i=1}^{p} \phi_i * y_{t-i} - \sum_{j=1}^{q} \theta_j * e_{t-j}$
\end{center}

\noindent where $\hat{y_t}$ refer to the predicted value at time t, $\mu$ refer to a constant, while $\sum_{i=1}^{p} \phi_i * y_{t-i}$ refers to the AR terms and $\sum_{j=1}^{q} \theta_j * e_{t-j}$ refers to the MA terms.

\subsection{Baseline ARIMA Result}
As a baseline model, each of the locations was assessed individually and a suitable ARIMA model was built for each location. Auto-arima function from Python was used to implement this. The out-of-sample MSE for this baseline model on the smaller dataset is \textbf{47.60} and MSE for dataset of all locations is \textbf{73.29}. A sample forecast plot is shown below:

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Images/forecast_example.jpg}
    \textbf{\caption{ARIMA forecast on a random location}}
    \label{fig:ARIMA forecast on a random location}
\end{figure}

\section{GLM Model}
The dataset that we are using follows a count time series, which means the observations are non-negative integers. A flexible and commonly used model for count time series is the Generalized Linear Model (GLM) \cite{Nelder1972}. GLM assumes that each value is conditionally dependent on its past values. According to \cite{Tobias2017}, an advantage of GLM models over typical ARIMA models is that GLM can describe covariate effects and negative correlations in a more direct way. GLM normally take the form of:

\begin{center}
    $\displaystyle g(\lambda_t)= \eta^T X_t$
\end{center}

\noindent where $g$ represents a link function, and $\lambda_t = E(Y_t | F_{t-1})$, where $F_{t-1}$ represents the historical values up to time t, which is the conditional mean of the time series . $\eta$ represents a parameter vector that corresponds to the covariates. $X_t$ represents the a vector of the values of the previous time steps in the time series.

%\noindent Using the R package from \cite{Tobias2017}, the GLM used would be an %extension of the above equation and can be expressed in the form of:

%\begin{center}
%    $\displaystyle g(\lambda_t)=\beta_0 + \sum_{k=1}^{p}\beta_k\tilde{g}(Y_{t-i_k}) + \sum_{\mathit{l}=1}^{q}\alpha_\mathit{l} g(\lambda_{t-j_\mathit{l}}) + \eta^T X_t$
%\end{center}

%\noindent where additionally, $\tilde{g}$ represents a transformation function. (remove this?)

\subsection{Model Implementation}
\noindent As mentioned in section 3.2, we would first assess our model on the smaller dataset before testing it on the full dataset. Using the R package tscount from \cite{Tobias2017}, we fit each of the locations individually by a GLM model. For the parameters, we used the past 6 lagged values as well as the identity function as the link function, with poisson distribution as the conditional distribution. The out-of-sample MSE for this baseline model on the smaller dataset is \textbf{38.16}, which is better than just using ARIMA on the same dataset.  \\

\noindent The output for GLM model is in a fractional number instead of an integer. Since the output of the GLM model, using poisson distribution, gives the expected value, which might not be an integer. Hence, to convert our predictions to an integer for practical purposes, we round the predictions to the nearest integer since the nearest integer would be the value that is predicted to occur most often in the distribution. 
\noindent However, when it is implemented on the full dataset, the MSE of the GLM increases to \textbf{82.42}, which is worse than just applying ARIMA on every location, which gives a MSE of 73.29. 

\subsection{Model Diagnostics}
To validate and verify if our fitted model is adequate, model checking would be performed by performing the following residual analysis.		

\newpage
\subsubsection{Residuals plots}
\hfill\\
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=0.7\textheight]{Images/Full_GLM_resids.jpg}
    \caption{Residuals for GLM Model}
    \label{fig:Residuals for GLM Model}
\end{figure}

\noindent The diagnostic plots in Figure 8 shows that while most residuals are randomly scattered around -5 to 10, there are still many points above 10, and this also imply that the GLM model produces residuals that does not follow the normal distribution well. Although this is not ideal, this might be attributed to the demand data following a poisson distribution instead of a normal distribution. 

\subsubsection{Residuals against Predicted values}
\hfill\\
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, height=0.5\textheight]{Images/Full_GLM_resids_vs_fitted.jpg}
    \caption{Residuals against Predicted values for GLM Model}
    \label{fig:Residuals against Predicted values for GLM Model}
\end{figure}
The above plot of residuals against the predicted values suggests heteroscedasticity between residuals, or non-constant variance between the residuals as the predicted value increases. This heteroscadasticity among residuals would sometimes occur in a Poisson GLM, as mentioned in \cite{Dylan2017}. 

\subsection{Model Limitation}
Similar to the baseline ARIMA model, it would be a relatively expensive and time-consuming process as every location has to be individually fitted to a GLM model. Also, each location model only uses its past values and does not take into account data from the other locations. The next section explores another type of model which would use data from other locations as predictors. 

\section{Vector Autoregressive (VAR) Model}

\subsection{Details of VAR Model}
Vector Autoregressive (VAR) models are the most commonly used model for multivariate time series, particularly in economics and financial time series as shown in \cite{Hilde2000}. VAR models are very similar to multivariate linear regression models and methods used to perform inferencing on linear regression models can also be applied to VAR models. VAR(p) represents a VAR model of order p if the time series can be written as: 

\begin{center}
    $\displaystyle y_t=v+\sum_{i=1}^{p} \phi_{i}y_{t-i}+\alpha_t$
\end{center}

\noindent where $p$ is the number of lagged endogenous variables used, $y_t$ is the value at time $t$, $v$ is a constant vector, $\phi_i$ are coefficient matrices for $i>0$ and $\alpha_t$ are independent and identically distributed random vectors. 

\subsection{Motivation}

As with many spatio-temporal demand prediction, spatial features could be an important feature if there exist spatial correlation. To check if there exist spatial correlation between the locations, a correlation heatmap would be plotted. For clear illustration purposes, instead of showing the matrix of correlation for all 839 locations, only the 42 locations with at least 50 non-zero counts would be shown. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=0.75\textheight]{Images/heatmap.jpg}
    \textbf{\caption{Spatial Correlation Heatmap on the locations with at least 50 non-zero counts. Axes represent the 42 locations}}
    \label{fig:Spatial Correlation Heatmap. Axes represent locations}
\end{figure}

\noindent This thus provides the motivation to use the existence of spatial correlation and explore the usage of VAR model.

\newpage
\subsection{Stationarity Condition}
For a univariate time series, it is important for the time series to be transformed into a stationary series and Augmented Dickey-Fuller (ADF) test can be used to perform unit root test for stationarity, as shown in \cite{Zhijie1998} and \cite{Rizwan2011}. For a multi-variate time series, if the series are unit-root non-stationary, regression models might show a statistially significant, but false, coefficients and results simply because the series are coincidentally increasing over time, hence applying the VAR model on non-stationary series could lead to spurious regression, also shown in \cite{Baumohl2009}.  


\subsection{Implementation of VAR Model for our dataset}

To test for stationarity, we would be performing the standard ADF test to test for stationarity. The result of the ADF test showed that time series of certain locations are not stationary. We then take the first difference of each series and after differencing, ADF test was performed on each series the time series of each location have been checked and are stationary. It is noted that while it is possible to perform differencing on every series, it might cause over-differencing, leading to inaccurate results, as mentioned in \cite{Ruey2014}. \\

\noindent After we applied a differencing order of 1 to the dataset, BigVAR Library in R was used to fit a VAR model with a selected p of 12, representing the usage of 12 lagged values, using time series of every location as predictors. The results for the VAR model is discussed in the later subsection. 


\subsubsection{Cointegration}
\hfill\\
\cite{Box1977} shows that it is possible to linearly combine various unit-root nonstationary time series to form a stationary series. The term Cointegration, first mentioned in \cite{Granger1983}, states that although some or all the time series might be unit-root nonstationary individually, these time series can be said to be cointegrated if there exists a possible linear combination of them that would form a stationary series. Intuitively, 2 series are cointegrated if they move together and the distance between them remain stable over time. 

\subsubsection{Johansen Test for Cointegration}
\hfill\\
While Cointegrated Augmented Dickey Fuller Test, commonly used for Pairs Trading, can be used, it is able to be applied on only 2 separate series. The next best approach is the cointegrating tests for VAR model, called the Johansen's Cointegration Test. However, one limitation is that it can only be used to check for cointegration between a maximum of 12 variables. For further elaboration on the Johansen's Cointegration Test, please refer to \cite{Johansen1991}. If there exists cointegration between variables, a Vector Error Correction Model can be formulated. \\

\noindent However, since our dataset has 839 variables (locations), we are unable to apply the cointegration test or accurately calculate the significant values of more than 12 variables and hence unable to determine correctly the number of cointegration vectors needed. 

\subsection{VARX Model}
VAR models can also be extended to include exogenous variables. A VARX(p,s) (with exogenous variables) model can be expressed as:

\begin{center}
    $\displaystyle y_t=v+\sum_{i=1}^{p} \phi_{i}y_{t-i}+\sum_{j=1}^{s} \beta_{j}x_{t-j}+\alpha_t$
\end{center}

\noindent where $p$ is the number of lagged endogenous variables used, $s$ is the number of lagged exogenous variables used, $y_t$ is the value at time $t$, $v$ is a constant vector, $\phi_i$ are coefficient matrices for endogenous coefficient matrix for $i>0$, $\beta_i$ are coefficient matrices for exogenous coefficient matrix for $i>0$ and $\alpha_t$ are independent and identically distributed random vectors. 

\subsection{Implementation of VARX Model on our dataset}
\noindent Our dataset uses additional exogenous variables like temperature, wind, gust, cloud, humidity, precipitation, pressure as well as one-hot encoding of the day of the week. Our dataset now would have 839 endogenous variables/locations and 13 exogenous variables. Based on performance results and taking computational power limitation into consideration, the optimal number of lag (p) for endogenous variables is 6 and number of lag for exogenous variables is 1. \\

\noindent Similar to VAR above, BigVAR library was used to fit a VARX model taking in the 839 locations' time series as endogenous variables and the 13 exogenous variables. The model was then use to compute predictions for the 6 periods ahead and would be compared with the test set.

\newpage
\subsection{Model Checking}
The following residual analysis are then performed:


\subsubsection{Whiteness of Residuals}
\hfill\\
To ensure our fitted model is adequate, the resiudals should behave like a white noise series. 
The plots below shows the distribution of our residuals for the VAR model and the VARX model. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, height=0.42\textheight]{Images/Full_VAR_diff_resids.jpg}
    \caption{Residuals for VAR Model}
    \label{fig:Residuals for VAR Model}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, height=0.42\textheight]{Images/Full_VARX_diff_resids.jpg}
    \caption{Residuals for VARX Model}
    \label{fig:Residuals for VARX Model}
\end{figure}

\noindent From Figure 11 and 12, we can see that the residuals are mostly randomly scattered and they roughly follow a normal distribution, although it performs rather badly on the lower end and higher end of the outliers. 


%\subsubsection{Residuals against Fitted Values}
%
%\begin{figure}[H]
%    \centering
%    \includegraphics[width=0.6\textwidth, height=0.3\textheight]{Images/Full_VAR_diff_resids_vs_values.jpg}
%    \caption{Residuals against Fitted Values for VAR Model}
%    \label{fig:Residuals against Fitted Values for VAR Model}
%\end{figure}
%
%\begin{figure}[H]
%    \centering
%    \includegraphics[width=0.6\textwidth, height=0.3\textheight]{Images/Full_VARX_diff_resids_vs_values.jpg}
%    \caption{Residuals against Fitted Values for VARX Model}
%    \label{fig:Residuals against Fitted Values for VARX Model}
%\end{figure}
%
%Figure 13 and 14 shows that heteroscadasticity occurs at the lower and higher ends of the predicted values, whereas in the middle range, the residuals tend to be roughly randomly scattered.

\subsection{Results}
On the small dataset, the MSE using the VAR model is \textbf{47.28} while the MSE using the VARX model is \textbf{45.76}. On the full dataset, the MSE using the VAR model  is \textbf{82.87} while the MSE using the VARX model is \textbf{68.29}.Since the VARX includes exogenous variables and have a better result compared to the VAR model, it shows that the exogenous variables used do have a moderate amount of explanatory power. 

\subsection{Model results and limitations}

We also note that the output for VAR or VARX model is in a fractional number instead of an integer. To convert our predictions to an integer for practical purposes, we round the predictions to the nearest integer. While both VAR and VARX are an improvement from the GLM model and also beats the ARIMA baseline model on the smaller dataset, the VAR model fails to beat the baseline model MSE of 73.29. This might be because in the baseline model, since each location is fitted with an individual ARIMA model, those sparse locations would have predictions of all 0, which would be accurate. In the smaller dataset, since we only used locations that are not sparse (locations with at least 50 non-zero counts), and the VAR/VARX model uses all locations' demand values, the existence of spatial correlation would cause the VAR/VARX model to perform better than the ARIMA. However, using VAR/VARX on the full dataset, which contains locations with sparse demand would not beat the ARIMA model because the presence of non-zero coefficients would cause many predictions to be non-zero, hence producing a more inaccurate prediction as compared to the baseline ARIMA model. \\

%\noindent Having noticed the abovementioned problem in locations with sparse demand. the next section would introduce a 3-step approach to fix this issue. 

\newpage

\section{Spatio-Temporal Autoregressive Model}

\cite{Abolfazl2017} has evaluated ARMA, VAR and Spatio-Temporal Autoregressive (STAR) model to predict taxi demands in New York City and have found that STAR models performs much better. This section would explore the use of STAR models on our dataset. \\

\noindent \cite{KURT20152537} 


\section{3-step Approach}

In this section, we introduce an alternative 3-step approach. The overall steps are summarised here:

\noindent \textbf{Step 1}. Perform Clustering on the time series using one of the distance metric: 

	1.1 Euclidean distance between geo-location coordinates
	
	1.2 Correlation between time series 
	
	1.3 Dynamic Time Warping (DTW) distance between time series \\
	
	
	
\noindent \textbf{Step 2}. Aggregate the clusters by performing a summation the respective locations' demand over time. Train a VAR model on the clusters to predict the total demand for each cluster. \\

\noindent \textbf{Step 3}. Re-allocate the total demand of each cluster to each location in the cluster while maintain a relative constant distribution as before. \\






%\begin{algorithm}
%\caption{3-Step Approach}
%\begin{algorithmic}
%\REQUIRE $n \geq 0 \vee x \neq 0$
%\ENSURE $y = x^n$
%\STATE 1. Cluster the time series.
%\IF{$n < 0$}
%\STATE $X \leftarrow 1 / x$
%\STATE $N \leftarrow -n$
%\ELSE
%\STATE $X \leftarrow x$
%\STATE $N \leftarrow n$
%\ENDIF
%\WHILE{$N \neq 0$}
%\IF{$N$ is even}
%\STATE $X \leftarrow X \times X$
%\STATE $N \leftarrow N / 2$
%\ELSE[$N$ is odd]
%\STATE $y \leftarrow y \times X$
%\STATE $N \leftarrow N - 1$
%\ENDIF
%\ENDWHILE
%\end{algorithmic}
%end{algorithm}

\noindent By performing clustering and aggregation of the locations before reallocating the demand to each locations based on its past distribution, the predicted demand for each location would not purely be attained from past values of all individual locations. Instead, the clustering would reduce the total dimensionality of the data when being fitted to a VAR to attain a more reasonable cluster demand prediction, before being reallocated to the locations based on its past distribution and this might provide a better prediction for locations with sparse demands. Similar methods can be found in \cite{Paul2015} and \cite{Chi2014}.

\noindent 
\subsection{Clustering}

A simple clustering was first done on the locations using their time series data in the training set. 3 different kind of distance metrics were used to perform K-means clustering. The first metric used would be based on their geo-locations while the second and third metric used would be to cluster them based on similarity between their historical time series. 

\subsubsection{Using longitude/latitude as the distance metric}
\hfill\\
A reasonable assumption that could be made is that customers in the same neighbourhood would tend to have similar ordering behaviour due to similar residential status or similar working industry or culture. Hence, we would cluster the locations using their geo-location coordinates (longitude and latitude). K-means clustering is performed using the euclidean distance of the locations as the distance metric.   

\subsubsection{Using Correlation Coefficient as the distance metric}
\hfill\\
We then also explored using correlation coefficient between the time series as the distance metric for K-means clustering. Correlation coefficient can be calculated by: 

\begin{center}
$r_{xy} = \frac{\sum_{i=1}^{n} (x_i - \overline{x})(y_i - \overline{y})}
{\sqrt{\sum_{i=1}^{n} (x_i - \overline{x})^2(y_i - \overline{y})^2}} $
\end{center}

\noindent where $r_{xy}$ represents the correlation coefficient between time series x and y, $x_i$ and $y_i$ represents points in time i for time series x and y respectively and $\overline{x}$ and $\overline{y}$ represents the mean value for time series x and y. Correlation measures the strength and direction of the tendency for any 2 time series to move together. 

\subsubsection{Using Dynamic Time Warping as the distance metric}
\hfill\\
Finally, we explored using Dynamic Time Warping (DTW) Distance as the distance metric for Hierachical clustering. \\

\noindent DTW is one of the commonly-used algorithm for speech or audio recognition and it measures the similarity between time series by providing a elastic non-linear alignment between 2 time series. It is known for being effective for time series that have different length or speed. DTW is calculated by first creating a distance matrix and then finding the optimal warping path as shown in the 2 algorithms here:

\begin{algorithm}[H]
\caption{Dynamic Time Warping (DTW) Algorithm to form distance matrix}
\begin{algorithmic}
%\REQUIRE $n \geq 0 \vee x \neq 0$
%\ENSURE $y = x^n$
\STATE 
\STATE 1. Initialise a 2-dimensional matrix M, where the indices of the rows and indices of the columns represent each point in time series x and time series y. 
\STATE 2. Populate the matrix from bottom-left to top-right with each element $c_{i,j}$ of the matrix representing the distance between $x_i$, the $i^{th}$ element of time series x and $y_j$, the $j_{th}$ element of time series y, which is calculated by:

\begin{center}
$c_{i,j} = (x_i - y_j) + min(c_{i-1,j}, c_{i,j-1}, c{i-1,j-1})$
\end{center}

\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Using DTW Matrix to find optimal warping path}
\begin{algorithmic}
\REQUIRE distance matrix of dimension i*j obtained from DTW Algorithm
%\ENSURE $y = x^n$
\STATE Let i = rows(matrix) and j = columns(matrix)
\STATE Let path = []
\WHILE {(i != 1) and (j != 1)}
\IF{i==1}
\STATE $j = j-1$
\ELSIF{j==1}
\STATE $i = i-1$
\ELSE
\IF{matrix[i-1,j] == min(matrix(i − 1, j), matrix(i, j − 1), matrix(i − 1, j − 1))}
\STATE $i = i - 1$
\ELSIF{matrix[i,j-1] == min(matrix(i − 1, j), matrix(i, j − 1), matrix(i − 1, j − 1))}
\STATE $j = j - 1$
\ELSE
\STATE $i = i - 1, j = j - 1$
\ENDIF
path.add((i,j))
\ENDIF
\ENDWHILE
\RETURN path
\end{algorithmic}
\end{algorithm}


%\IF{$n < 0$}
%\STATE $X \leftarrow 1 / x$
%\STATE $N \leftarrow -n$
%\ELSE
%\STATE $X \leftarrow x$
%\STATE $N \leftarrow n$
%ENDIF
%WHILE{$N \neq 0$}
%\IF{$N$ is even}
%\STATE $X \leftarrow X \times X$
%\STATE $N \leftarrow N / 2$
%\ELSE[$N$ is odd]
%\STATE $y \leftarrow y \times X$
%\STATE $N \leftarrow N - 1$
%\ENDIF
%\ENDWHILE
%\end{algorithmic}
%\end{algorithm}
\newpage
\noindent We can also understand DTW visually by:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth, height=0.5\textheight]{Images/DTW_Plot.jpg}
    \textbf{\caption{DTW Alignment Plot}}
    \label{fig:DTW Alignment Plot}
\end{figure}

\noindent The plot in the middle shows the optimal warping path between the 2 time series. The distance is calculated by the euclidean distance between the points of the 2 time series that lie along the warping path as shown in:

\begin{center}
$DTW Distance = \sqrt{\sum_{(i,j)\in {Warping Path}}(x_i - y_j)^2}$
\end{center}

\noindent As shown in \cite{Hui2008}, DTW is also used commonly as a time series similarity measure for time series classification and it can perform as well as other state-of-the-art measures. More details on the applications and optimisation of DTW can also be found in \cite{Pavel2008}  
\newpage 

\subsection{Predicting total demand for each cluster}

\noindent We first aggregate the locations of each cluster together such that:

\begin{center}
$x\:\epsilon \: C(i)$
\end{center}

\begin{center}
$D_{C(i)} ={\sum_{l\epsilon C(i)}\sum_{j=1}^{n}x_{lj}}$ 
\end{center}

\noindent if location x belongs to Cluster i and where $D_{C(i)}$ represents the total demand (from training set) of the cluster C(i) and $x_{ij}$ represents the training set demand at location i at time j. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth, height=0.62\textheight]{Images/cluster_corr_heatmap.jpg}
    \textbf{\caption{Correlation heatmap for 9 clusters}}
    \label{fig:Correlation heatmap for 9 clusters}
\end{figure}

\noindent We can observe from the sample correlation heatmap in Figure 16 that there exist strong correlation between certain clusters (like cluster 3 and 4 and cluster 6 and 8). To exploit the spatial correlation effects, similar steps as mentioned in Section 6 was used. 

1. We first treat each cluster as a variable, where each cluster has a time series which is a vector of demands, calculated as the sum of the demand of all locations in the cluster, as shown above as $D_{C(i)}$. 

2. Next, we check for stationarity for all the cluster's time series and perform differencing if they are non-stationary. 

3. A VAR model was then trained using the clusters as variables.

4. If we have performed differencing in step 2, we then use the VAR model to predict the differenced values for time-step 199 to 204. We then add back the difference to the previous values to convert it back to the original demand for each cluster. \\

\noindent If we want to use a VARX model,the exogenous variables of each locations would have to be aggregated in some way when we combine the different locations into clusters, which might introduce a additional source of error. Hence, in this section, we would only use VAR since it does not require the aggregation of exogenous variables.


\subsection{Assigning total cluster demand to individual location}

After we attained the total demand for each cluster, the next step would be to reallocate the total demand to each individual location in the cluster, making sure that the distribution of demand across the locations are relatively similar to as before. \\

\noindent One assumption that was made here is that the distribution of the demand across the locations in each cluster remain relatively constant over time. Taking the previous distribution into account, the predicted demand for each cluster would then be reallocated to each individual location in the cluster using this equation:

\begin{center}
    $\displaystyle y_{i} = Y_{C(i)} * \frac{(\sum_{j=1}^{n}x_{ij})}{D_{C(i)}} \: \forall \: i$
\end{center}

\noindent where $y_i$ represents the predicted demand for location i, $Y_{C(i)}$ represents the predicted total demand of Cluster i, $C(i)$ represents the cluster which location i belongs to, $D_{C(i)}$ represents the total demand (from training set) of the cluster C(i).


\subsection{Results}

In this 3-step approach, it is not appropriate to use conventional methods, such as the elbow method, to determine the number of clusters. This is because our purpose here is not to minimize the within cluster sum-of-squares, but rather to find the optimal combination of groups that yields the best forecast accuracy after we fit a VAR model on them and reallocate the demand using the VAR model predictions. Hence, we tried different number of clusters for the 3 different distance metric as mentioned in Section 7.1 and observe the MSE for each of those.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth, height=0.6\textheight]{Images/Cluster_Loc_MSE_plot.jpg}
    \textbf{\caption{Plot of MSE against number of clusters. Distance metric used: Euclidean Distance of geographical locations. The optimal number of clusters is 3 with a MSE of 60.10.}}
    \label{fig:Plot of MSE against number of clusters. Distance metric used: Euclidean Distance of geographical locations. The optimal number of clusters is 3 with a MSE of 60.10.}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth, height=0.6\textheight]{Images/Cluster_Corr_MSE_plot.jpg}
    \textbf{\caption{Plot of MSE against number of clusters. Distance metric used: Correlation of time series. The optimal number of clusters is 11 with a MSE of 59.76.}}
    \label{fig:Plot of MSE against number of clusters. Distance metric used: Correlation of time series. The optimal number of clusters is 11 with a MSE of 59.76.}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth, height=0.6\textheight]{Images/Cluster_DTW_MSE_plot.jpg}
    \textbf{\caption{Plot of MSE against number of clusters. Distance metric used: DTW of time series. The optimal number of clusters is 8 with a MSE of 60.36.}}
    \label{fig:Plot of MSE against number of clusters. Distance metric used: Correlation of time series. The optimal number of clusters is 8 with a MSE of 60.36.}
\end{figure}

%\begin{table}[H]
%\begin{tabular}{|l|l|l|l|l|}
%\hline
%                                                                                                                          & \textbf{3 Clusters} & \textbf{6 Clusters} & \textbf{9 Clusters} & \textbf{12 Clusters} \\ \hline
%\textbf{\begin{tabular}[c]{@{}l@{}}Clustering using euclidean\\ distance of latitude/longitude\end{tabular}}              & 64.39               & 63.00      & \textbf{60.12}               & 63.39                \\ \hline
%\textbf{\begin{tabular}[c]{@{}l@{}}Clustering using correlation \\ between time series as distance\\ metric\end{tabular}} & 63.45               & 63.97               & \textbf{59.27}              & 63.27       \\ \hline
%\textbf{\begin{tabular}[c]{@{}l@{}}Clustering using DTW as\\ distance metric\end{tabular}}                                & 66.39               & \textbf{62.07}      & 65.67 & 62.56                
%\\ \hline
%\end{tabular}
%\textbf{\caption{MSE for different clustering distance metrics and number of clusters. Bolded results signifying the best result for the respective distance metric.}}
%\end{table}

\noindent Based on the results, clustering the time series into 11 groups using the correlation between time series as the distance metric seems to give the best MSE of \textbf{59.76}. 

%\subsubsection{Model Diagnostics}
%
%To test the adequacy of our model fitness, particularly our VAR model, we will plot the residuals against the fitted values and observe if there are signs of heteroscedasticity. 
%
%\begin{figure}[H]
%    \centering
%    \includegraphics[width=\textwidth, height=0.6\textheight]{Images/corr_var_res_fit_plot.jpg}
%    \textbf{\caption{Plot of residuals against fitted values for VAR model}}
%    \label{fig:Plot of residuals against fitted values for VAR model}
%\end{figure}
%
%\noindent From the plot of residuals against the fitted values of the VAR model, we observe that although there seem to be a concentration of residuals around the predicted values of -5 to 5, there is no clear sign of heteroscedasticity, hence our model is reasonably adequate.

\newpage
\section{Results and Conclusion}
The table below summarises the results of our models. 

\begin{table}[H]
\begin{tabular}{|l|l|l|ll}
\cline{1-3}
                                                                                                             & \textbf{\begin{tabular}[c]{@{}l@{}}MSE (Locations with at \\ least 50 non-zero counts)\end{tabular}} & \textbf{MSE (All locations)} &  &  \\ \cline{1-3}
\textbf{ARIMA}                                                                                               & 47.60                                                                                              & 73.29                         &  &  \\ \cline{1-3}
\textbf{GLM}                                                                                                 & 38.16                                                                                              & 82.42                         &  &  \\ \cline{1-3}
\textbf{VAR}                                                                                                 & 47.28 & 82.87                        &  &  \\ \cline{1-3}
\textbf{VARX}                                                                                                & 45.76                                                                                              & 68.29                         &  &  \\ \cline{1-3}
\textbf{\begin{tabular}[c]{@{}l@{}}3-step approach (Using K-means \\ on geographical location)\end{tabular}} & 40.42 & 60.12 &  &  \\ \cline{1-3}
\textbf{\begin{tabular}[c]{@{}l@{}}3-step approach (Using \\ K-means on correlation \\ of time series)\end{tabular}}           & 42.83 & \textbf{59.27}                         &  &  \\ \cline{1-3}
\textbf{\begin{tabular}[c]{@{}l@{}}3-step approach (Using \\ K-means on DTW \\ of time series)\end{tabular}}           & 39.05 & 62.07                         &  &  \\ \cline{1-3}
\end{tabular}
\textbf{\caption{MSE of the different methods used in this paper}}
\end{table}


\noindent VAR and VARX model performs better than the GLM models on the full dataset, suggesting that the spatial relationship between locations are useful in forecasting. Also, VARX performs better than VAR model in both datasets, suggesting the exogenous variables have some explanatory power and do improve the forecast accuracy. \\

\noindent Applying the 3-step approach also gives a much improved result as compared to just using VAR on all the locations. This might be due to the 3-step approach being more reasonable in dealing with locations with sparse demand due to the aggregation of clusters in the clustering method. In the 3-step approach, our VAR model uses just the 11 clusters as variables to predict the total cluster demand for just 11 clusters, while the original VAR uses all 839 locations as variables to predict for 839 locations, which is more likely to produce a higher error rate. This is similar to the findings from \cite{Abolfazl2017}, which states that a simple VAR model would not perform as well for high-dimensional data. \\

\noindent As for the distance metric used for clustering, using correlation also gives better results than using euclidean distance between the geo-locations, and this would imply that spatial correlation is not limited to just purely geographical proximity but also on the time series similarity. Comparing correlation measure and DTW measure, using correlation between the time series as the distance metric gives us the best result. Correlation compares the general shape and trend of the time series regardless of scale, whereas for DTW, it is more effective to compare time series of varying length and speed. 

\section{Limitations and Future Work}
Our 3-step approach model display decent MSE result. However, using VAR is still technically a linear model and future work could include exploring using non-linear models like neural network, such as Long Short-Term Memory (LSTM) models or Convolutional LSTM models that also uses spatial-temporal features. Neural network models are also believed to be able to take into account the non-stationarity of the time series, which would allow our model to be more flexible without the need for differencing. \\

\noindent Another area that could be explored further would be the method used to assign the total cluster demand to individual locations. Our current method simply used the historical distribution of the total sum of demand of each location over all the locations in that cluster. This would work well if the distribution of the demand for the locations remain relatively constant over time but if the distribution of the locations fluctuates greatly, the current reassignment formula might not be a suitable model. Instead, we could attempt to create another model to predict the distribution of the locations in each cluster in order to get a relatively more stable and reasonable distribution over time. 

\newpage



% Appendix here
% Options are (1) APPENDIX (with or without general title) or
%             (2) APPENDICES (if it has more than one unrelated sections)
% Outcomment the appropriate case if necessary
%
% \begin{APPENDIX}{<Title of the Appendix>}
% \end{APPENDIX}
%
%   or
%
% \begin{APPENDICES}
% \section{<Title of Section A>}
% \section{<Title of Section B>}
% etc
% \end{APPENDICES}

%%


% Acknowledgments here
%\ACKNOWLEDGMENT{}
\section{Appendix}
\subsection{Distribution Plots of Exogenous Variables}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth, height=0.3\textheight]{Images/distplot_cloud.png}
    \textbf{\caption{Distribution of Cloud}}
    \label{fig:Distribution of Cloud}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth, height=0.3\textheight]{Images/distplot_gust.png}
    \textbf{\caption{Distribution of Gust}}
    \label{fig:Distribution of Gust}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth, height=0.3\textheight]{Images/distplot_humidity.png}
    \textbf{\caption{Distribution of Humidity}}
    \label{fig:Distribution of Humidity}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth, height=0.3\textheight]{Images/distplot_pressure.png}
    \textbf{\caption{Distribution of Pressure}}
    \label{fig:Distribution of Pressure}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth, height=0.3\textheight]{Images/distplot_temp.png}
    \textbf{\caption{Distribution of Temperature}}
    \label{fig:Distribution of Temperature}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth, height=0.3\textheight]{Images/distplot_wind.png}
    \textbf{\caption{Distribution of Wind}}
    \label{fig:Distribution of Wind}
\end{figure}

\subsection{Mean Demand Against Exogenous Variables}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth, height=0.3\textheight]{Images/wind_mean_demand.jpg}
    \textbf{\caption{Mean Demand against Wind}}
    \label{fig:Mean Demand against Wind}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth, height=0.3\textheight]{Images/gust_mean_demand.jpg}
    \textbf{\caption{Mean Demand against Gust}}
    \label{fig:Mean Demand against Gust}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth, height=0.3\textheight]{Images/cloud_mean_demand.jpg}
    \textbf{\caption{Mean Demand against Cloud}}
    \label{fig:Mean Demand against Cloud}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth, height=0.3\textheight]{Images/humidity_mean_demand.jpg}
    \textbf{\caption{Mean Demand against Humidity}}
    \label{fig:Mean Demand against Humidity}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth, height=0.3\textheight]{Images/prec_mean_demand.jpg}
    \textbf{\caption{Mean Demand against Precipitation}}
    \label{fig:Mean Demand against Precipitation}
\end{figure}

\subsection{MSE result for different cluster size and distance metric}

\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Cluster Size} & \textbf{Euclidean Distance} & \textbf{Correlation between time series} & \textbf{DTW Distance} \\ \hline
\textbf{1}                  & 68.67                                              & 68.67                                               & -                                \\ \hline
\textbf{2}                  & 69.71                                              & 68.66                                               & 62.96                            \\ \hline
\textbf{3}                  & \textbf{60.10}                                     & 61.97                                               & 61.49                            \\ \hline
\textbf{4}                  & 62.09                                              & 61.01                                               & 61.28                            \\ \hline
\textbf{5}                  & 62.20                                              & 66.43                                               & 61.00                            \\ \hline
\textbf{6}                  & 62.04                                              & 65.25                                               & 61.40                            \\ \hline
\textbf{7}                  & 62.26                                              & 62.86                                               & 60.73                            \\ \hline
\textbf{8}                  & 63.97                                              & 63.07                                               & \textbf{60.36}                   \\ \hline
\textbf{9}                  & 64.69                                              & 64.31                                               & 60.55                            \\ \hline
\textbf{10}                 & 61.09                                              & 60.57                                               & 61.37                            \\ \hline
\textbf{11}                 & 61.80                                              & \textbf{59.76}                                      & 63.44                            \\ \hline
\textbf{12}                 & 64.07                                              & 61.42                                               & 66.15                            \\ \hline
\end{tabular}
\textbf{\caption{MSE with different cluster size and distance metric}}
\end{table}
\newpage

% References here (outcomment the appropriate case)
\bibliographystyle{ormsv080}
%%%\bibliographystyle{abbrvnat}
\bibliography{ref}
% CASE 1: BiBTeX used to constantly update the references
%   (while the paper is being written).
%\bibliographystyle{ormsv080} % outcomment this and next line in Case 1
%\bibliography{<your bib file(s)>} % if more than one, comma separated

% CASE 2: BiBTeX used to generate mypaper.bbl (to be further fine tuned)
%\input{mypaper.bbl} % outcomment this line in Case 2

%If you don't use BiBTex, you can manually itemize references as shown below.


\end{document} 
